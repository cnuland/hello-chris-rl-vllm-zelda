apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: qwen25-7b
  annotations:
    opendatahub.io/hardware-profile-name: gpu-1x-profile
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    opendatahub.io/model-type: generative
    openshift.io/display-name: qwen25-7b
    security.opendatahub.io/enable-auth: 'false'
  labels:
    app.kubernetes.io/part-of: zelda-rl
    zelda-rl/judge-role: text
spec:
  replicas: 1
  model:
    uri: hf://Qwen/Qwen2.5-7B-Instruct
    name: Qwen/Qwen2.5-7B-Instruct
  router:
    scheduler:
      template:
        containers:
        - name: main
          env:
          - name: TOKENIZER_CACHE_DIR
            value: /tmp/tokenizer-cache
          - name: HF_HOME
            value: /tmp/tokenizer-cache
          - name: TRANSFORMERS_CACHE
            value: /tmp/tokenizer-cache
          - name: XDG_CACHE_HOME
            value: /tmp
          args:
          - --pool-group
          - inference.networking.x-k8s.io
          - '--pool-name'
          - '{{ ChildName .ObjectMeta.Name `-inference-pool` }}'
          - '--pool-namespace'
          - '{{ .ObjectMeta.Namespace }}'
          - '--zap-encoder'
          - json
          - '--grpc-port'
          - '9002'
          - '--grpc-health-port'
          - '9003'
          - '--secure-serving'
          - '--model-server-metrics-scheme'
          - https
          - '--config-text'
          - |
            apiVersion: inference.networking.x-k8s.io/v1alpha1
            kind: EndpointPickerConfig
            plugins:
            - type: single-profile-handler
            - type: queue-scorer
            - type: kv-cache-utilization-scorer
            - type: prefix-cache-scorer
            schedulingProfiles:
            - name: default
              plugins:
              - pluginRef: queue-scorer
                weight: 2
              - pluginRef: kv-cache-utilization-scorer
                weight: 2
              - pluginRef: prefix-cache-scorer
                weight: 3
          volumeMounts:
          - name: tokenizer-cache
            mountPath: /tmp/tokenizer-cache
        volumes:
        - name: tokenizer-cache
          emptyDir: {}
    route: {}
    gateway: {}
  template:
    runtimeClassName: nvidia
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
    containers:
    - name: main
      env:
      - name: VLLM_ADDITIONAL_ARGS
        value: "--disable-uvicorn-access-log --max-model-len=4096 --enable-prefix-caching --served-model-name=qwen25-7b"
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            name: llm-d-hf-token
            key: HF_TOKEN
      resources:
        limits:
          cpu: '4'
          memory: 32Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '4'
          memory: 32Gi
          nvidia.com/gpu: '1'
      livenessProbe:
        httpGet:
          path: /health
          port: 8000
          scheme: HTTPS
        initialDelaySeconds: 120
        periodSeconds: 30
        timeoutSeconds: 30
        failureThreshold: 5
