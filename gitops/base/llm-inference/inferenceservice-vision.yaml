apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: qwen25-vl-32b
  labels:
    app: qwen25-vl-32b
    app.kubernetes.io/part-of: zelda-rl
    llm-d.ai/model: qwen25-vl-32b
    llm-d.ai/role: decode
  annotations:
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
spec:
  predictor:
    serviceAccountName: llm-inference-sa
    minReplicas: 3
    maxReplicas: 3
    containers:
    - name: kserve-container
      image: ghcr.io/llm-d/llm-d:v0.2.0
      command:
      - python
      - -m
      - vllm.entrypoints.openai.api_server
      args:
      - --model=Qwen/Qwen2.5-VL-32B-Instruct
      - --host=0.0.0.0
      - --port=8080
      - --tensor-parallel-size=2
      - --enable-prefix-caching
      - --block-size=16
      - --gpu-memory-utilization=0.9
      - --max-model-len=4096
      - --max-num-seqs=32
      - --trust-remote-code
      - --disable-log-requests
      - --kv-cache-dtype=auto
      - --served-model-name=qwen25-vl-32b
      env:
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            name: llm-d-hf-token
            key: HF_TOKEN
      - name: HF_HOME
        value: /tmp/huggingface
      - name: TRANSFORMERS_CACHE
        value: /tmp/huggingface/transformers
      - name: NCCL_DEBUG
        value: WARN
      ports:
      - containerPort: 8080
        name: h2c
        protocol: TCP
      resources:
        requests:
          cpu: "8"
          memory: "64Gi"
          nvidia.com/gpu: "2"
        limits:
          cpu: "16"
          memory: "120Gi"
          nvidia.com/gpu: "2"
      volumeMounts:
      - mountPath: /tmp/huggingface
        name: cache-volume
      - mountPath: /dev/shm
        name: shm
      readinessProbe:
        httpGet:
          path: /v1/models
          port: 8080
        initialDelaySeconds: 180
        periodSeconds: 30
        timeoutSeconds: 10
        failureThreshold: 10
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 300
        periodSeconds: 60
        timeoutSeconds: 10
        failureThreshold: 3
    nodeSelector:
      node-role.kubernetes.io/llm-inference: ""
    tolerations:
    - key: llm-inference
      operator: Equal
      value: "true"
      effect: NoSchedule
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
    volumes:
    - name: cache-volume
      emptyDir:
        sizeLimit: 200Gi
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 16Gi
